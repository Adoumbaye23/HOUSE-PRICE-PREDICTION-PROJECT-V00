

















import pandas as pd
import numpy as np
from scipy import stats
from sklearn import metrics
import joblib

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder

from sklearn.linear_model import LinearRegression, Ridge, Lasso,ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
from yellowbrick.regressor import ResidualsPlot
import missingno as msno


train_df = pd.read_csv("../data/train_df_labelled.csv")


train_df.head()


train_df.columns


train_df.select_dtypes("object")


# OverallQual= {
# 10: "Très excellent",
# 9: "Excellent",
# 8: "Très bon",
# 7: "Bon",
# 6: "Au-dessus de la moyenne",
# 5: "Moyenne",
# 4: "En dessous de la moyenne",
# 3: "Médiocre",
# 2: "Mauvais",
# 1: "Très mauvais"
# }"


""" train_df["OverallQual"] = train_df["OverallQual"].map(OverallQual) """


""" OverallCond= { 
10: "Très excellent",
9: "Excellent",
8: "Très bon",
7: "Bon",
6: "Au-dessus de la moyenne",
5: "Moyenne",
4: "En dessous de la moyenne",
3: "Médiocre",
2: "Mauvais",
1: "Très mauvais"
} """


""" train_df["OverallCond"] = train_df["OverallCond"].map(OverallCond) """


""" train_df.OverallQual """


""" ode_cols = ['OverallQual','OverallCond','LotShape', 'LandContour','Utilities','LandSlope',  'BsmtQual',  'BsmtFinType1',  'CentralAir',  'Functional', \
           'FireplaceQu', 'GarageFinish', 'GarageQual', 'PavedDrive', 'ExterCond', 'KitchenQual', 'BsmtExposure', 'HeatingQC','ExterQual', 'BsmtCond'] """

ode_cols=['Qualité globale', 'État général', 'Forme du lot', 'Topographie du terrain', 'Services publics disponibles', 
 'Inclinaison du terrain', 'Qualité du sous-sol', 'Type de finition sous-sol (1)', 'Climatisation centrale', 
 'Fonctionnalité globale', 'Qualité de la cheminée', 'Finition intérieure du garage', 'Qualité du garage', 
 'Allée pavée', 'État extérieur', 'Qualité de la cuisine', 'Exposition du sous-sol', 'Qualité du chauffage', 
 'Qualité extérieure', 'État général du sous-sol']


""" ohe_cols = ['Street', 'LotConfig','Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', \
           'MasVnrType','Foundation',  'Electrical',  'SaleType', 'MSZoning', 'SaleCondition', 'Heating', 'GarageType', 'RoofMatl'] """

ohe_cols=['Mois de vente','Type de logement','Type de route', 'Configuration du lot', 'Quartier', 'Proximité (1ère condition)', 'Proximité (2e condition)', 
 'Type de bâtiment', 'Style de maison', 'Style de toit', 'Revêtement extérieur (1)', 'Revêtement extérieur (2)', 
 'Type de parement en maçonnerie', 'Type de fondation', 'Système électrique', 'Type de vente', 
 'Zonage (Classification)', 'Condition de la vente', 'Type de chauffage', 'Type de garage', 'Matériau du toit']


len(ode_cols)


len(ohe_cols)


num_cols = train_df.select_dtypes(include=['int64', 'float64']).columns
num_cols = num_cols.drop('Prix de vente')
num_cols


num_pipeline = Pipeline(steps=[
    ('scaler', StandardScaler())
])


ode_pipeline = Pipeline(steps=[
    ('ode', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
])


ohe_pipeline = Pipeline(steps=[
    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])


col_trans = ColumnTransformer(transformers=[
    ('num_p', num_pipeline, num_cols),
    ('ode_p', ode_pipeline, ode_cols),
    ('ohe_p', ohe_pipeline, ohe_cols),
    ],
    remainder='passthrough', 
    n_jobs=-1)


pipeline = Pipeline(steps=[
    ('preprocessing', col_trans)
])


X = train_df.drop('Prix de vente', axis=1)
y = train_df['Prix de vente']


X_preprocessed = pipeline.fit_transform(X)


# TRAIN TEST SPLIT


X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=25)


def get_all_performances(value_train: tuple,
                         values_test: tuple,
                         metrics: list,
                        ) -> pd.DataFrame:
    """ Get all performance metrics listed in `metrics`
    
    Args:
        value_train (tuple): (y_train, y_train_pred)
        values_test (tuple): (y_test, y_test_pred)
        metrics (list): list of metrics to compute
    
    
    Returns:
        pd.DataFrame: column names are: ["metric", "train", "test"]
        
    """
    test_perfs = []
    train_perfs = []
    metric_names = []
    for metric_func in metrics:
        metric_name = metric_func.__name__
        metric_names.append(metric_name)
        train_perfs.append(metric_func(*value_train))
        test_perfs.append(metric_func(*values_test))
    perfs = {"metric": metric_names, "train": train_perfs, "test": test_perfs,}
    return pd.DataFrame(perfs)


METRICS = [metrics.r2_score,
           metrics.root_mean_squared_error,
           metrics.mean_absolute_percentage_error,
           metrics.max_error,
          ]


def get_input_features(model_pipeline, cat_step_name=None, ode_step_name=None):
    """
    Récupérer les noms des caractéristiques après transformation dans un pipeline.

    Parameters:
    - model_pipeline : pipeline complet du modèle avec un ColumnTransformer.
    - cat_step_name : nom de l'étape de prétraitement pour les variables catégorielles (optionnel).
    - ode_step_name : nom de l'étape de prétraitement pour les variables ordinales (optionnel).

    Returns:
    - Liste des noms de toutes les caractéristiques après transformation.
    """
    # Accéder au préprocesseur (ColumnTransformer) dans le pipeline
    col_trans = model_pipeline.named_steps['preprocessing']
    
    all_feature_names = []

    # Boucle sur chaque transformer dans le ColumnTransformer
    for step_name, pipe_trans, feat_names in col_trans.transformers_:
        if step_name == cat_step_name:
            # Récupérer les noms des caractéristiques après l'encodage OneHotEncoder
            if 'ohe' in pipe_trans.named_steps:  # Vérifie si le pipeline a un OneHotEncoder
                cat_feature_names = pipe_trans.named_steps['ohe'].get_feature_names_out(input_features=feat_names)
                all_feature_names.extend(cat_feature_names)
        elif step_name == ode_step_name:
            # Récupérer les noms des caractéristiques après l'encodage OrdinalEncoder
            if 'ode' in pipe_trans.named_steps:  # Vérifie si le pipeline a un OrdinalEncoder
                ode_feature_names = pipe_trans.named_steps['ode'].categories_[0]  # Cela renvoie les catégories de l'OrdinalEncoder
                all_feature_names.extend(ode_feature_names)
        else:
            # Ajouter les noms des caractéristiques pour les autres étapes (numériques)
            all_feature_names.extend(feat_names)

    # Retourner tous les noms des caractéristiques
    return all_feature_names




#build models








lr = LinearRegression()


lr.fit(X_train, y_train)


y_pred_lr = lr.predict(X_test)


res_viz = ResidualsPlot(lr,
                        is_fitted="auto",
                        qqplot=True,
                        hist=False,
                        train_color="blue",
                        test_color="red",
                       )
res_viz.fit(X_train, y_train)
res_viz.score(X_test, y_test)
res_viz.show(clear_figure=True);


# get performances in train & test
perform=get_all_performances(value_train=(y_train, lr.predict(X_train)),
                     values_test=(y_test, lr.predict(X_test)),
                     metrics=METRICS
                    )
perform


model_features = get_input_features(pipeline, cat_step_name="ohe_p", ode_step_name="ode_p")
model_features


#df_feature_importance = pd.DataFrame(
#    lr.coef_, 
#    columns=["coef"],
#    index=model_features # Les noms des caractéristiques
#)

#df_feature_importance.head()


# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/lr_performance.pkl")





ridge = Ridge()


param_grid_ridge = {
    'alpha': [0.05, 0.1, 1, 3, 5, 10],
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']
}


ridge_cv = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)


ridge_cv.fit(X_train, y_train)


np.sqrt(-1 * ridge_cv.best_score_)


perform=get_all_performances(value_train=(y_train, ridge_cv.predict(X_train)),
                     values_test=(y_test, ridge_cv.predict(X_test)),
                     metrics=METRICS
                    )
perform


# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/ridge_cv_performance.pkl")





Lasso = Lasso()


param_grid_Lasso = {
    'alpha': [0.05, 0.1, 1, 3, 5, 10]
}


Lasso_cv = GridSearchCV(Lasso, param_grid_Lasso, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)


Lasso_cv.fit(X_train, y_train)


np.sqrt(-1 * Lasso_cv.best_score_)


perform=get_all_performances(value_train=(y_train, Lasso_cv.predict(X_train)),
                     values_test=(y_test, Lasso_cv.predict(X_test)),
                     metrics=METRICS
                    )
perform


# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/Lasso_cv_performance.pkl")





ElasticNet = ElasticNet()


param_grid_ElasticNet = {
    'alpha': [0.05, 0.1, 1, 3, 5, 10],
    'l1_ratio': [0.1, 0.5, 0.9, 1]
}


ElasticNet_cv = GridSearchCV(ElasticNet, param_grid_ElasticNet, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)


ElasticNet_cv.fit(X_train, y_train)


np.sqrt(-1 * ElasticNet_cv.best_score_)


perform=get_all_performances(value_train=(y_train, ElasticNet_cv.predict(X_train)),
                     values_test=(y_test, ElasticNet_cv.predict(X_test)),
                     metrics=METRICS
                    )
perform



# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/ElasticNet_cv_performance.pkl")











RFR = RandomForestRegressor(random_state=13)


param_grid_RFR = {
    'max_depth': [5, 10, 15],
    'n_estimators': [100, 250, 500],
    'min_samples_split': [3, 5, 10]
}


rfr_cv = GridSearchCV(RFR, param_grid_RFR, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)


rfr_cv.fit(X_train, y_train)


np.sqrt(-1 * rfr_cv.best_score_)


rfr_cv.best_params_


# get performances in train & test
perform=get_all_performances(value_train=(y_train, rfr_cv.predict(X_train)),
                     values_test=(y_test, rfr_cv.predict(X_test)),
                     metrics=METRICS
                    )
perform


# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/rfr_cv_performance.pkl")





XGB = XGBRegressor(random_state=13)


param_grid_XGB = {
    'learning_rate': [0.05, 0.1, 0.2],
    'n_estimators': [300],
    'max_depth': [3],
    'min_child_weight': [1,2,3],
    'gamma': [0, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
}


xgb_cv = GridSearchCV(XGB, param_grid_XGB, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)


xgb_cv.fit(X_train, y_train)


np.sqrt(-1 * xgb_cv.best_score_)


perform=get_all_performances(value_train=(y_train, xgb_cv.predict(X_train)),
                     values_test=(y_test, xgb_cv.predict(X_test)),
                     metrics=METRICS
                    )
# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/xgb_cv_performance.pkl")





GBR = GradientBoostingRegressor()


param_grid_GBR = {
    'max_depth': [12, 15, 20],
    'n_estimators': [200, 300, 1000],
    'min_samples_leaf': [10, 25, 50],
    'learning_rate': [0.001, 0.01, 0.1],
    'max_features': [0.01, 0.1, 0.7]
}


GBR_cv = GridSearchCV(GBR, param_grid_GBR, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)


GBR_cv.fit(X_train, y_train)


np.sqrt(-1 * GBR_cv.best_score_)


perform=get_all_performances(value_train=(y_train, GBR_cv.predict(X_train)),
                     values_test=(y_test, GBR_cv.predict(X_test)),
                     metrics=METRICS
                    )
# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/GBR_cv_performance.pkl")





lgbm_regressor = lgb.LGBMRegressor()


param_grid_lgbm = {
    'boosting_type': ['gbdt', 'dart'],
    'num_leaves': [20, 30, 40],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 300]
}


lgbm_cv = GridSearchCV(lgbm_regressor, param_grid_lgbm, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)


lgbm_cv.fit(X_train, y_train)


np.sqrt(-1 * lgbm_cv.best_score_)


perform=get_all_performances(value_train=(y_train, lgbm_cv.predict(X_train)),
                     values_test=(y_test, lgbm_cv.predict(X_test)),
                     metrics=METRICS
                    )
# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/lgbm_cv_performance.pkl")





catboost = CatBoostRegressor(loss_function='RMSE', verbose=False)


param_grid_cat ={
    'iterations': [100, 500, 1000],
    'depth': [4, 6, 8, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.5]
}


cat_cv = GridSearchCV(catboost, param_grid_cat, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)


cat_cv.fit(X_train, y_train)


np.sqrt(-1 * cat_cv.best_score_)


perform=get_all_performances(value_train=(y_train, cat_cv.predict(X_train)),
                     values_test=(y_test, cat_cv.predict(X_test)),
                     metrics=METRICS
                    )
# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/cat_cv_performance.pkl")





vr = VotingRegressor([('gbr', GBR_cv.best_estimator_),
                      ('xgb', xgb_cv.best_estimator_),
                      ('ridge', ridge_cv.best_estimator_)],
                    weights=[2,3,1])


vr.fit(X_train, y_train)


y_pred_vr = vr.predict(X_test)


metrics.root_mean_squared_error(y_test, y_pred_vr)


perform=get_all_performances(value_train=(y_train, vr.predict(X_train)),
                     values_test=(y_test, vr.predict(X_test)),
                     metrics=METRICS
                    )
# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/vr_performance.pkl")





estimators = [
    ('gbr', GBR_cv.best_estimator_),
    ('xgb', xgb_cv.best_estimator_),
    ('cat', cat_cv.best_estimator_),
    ('lgb', lgbm_cv.best_estimator_),
    ('rfr', rfr_cv.best_estimator_),
]


stackreg = StackingRegressor(
            estimators = estimators,
            final_estimator = vr
)


stackreg.fit(X_train, y_train)


y_pred_stack = stackreg.predict(X_test)


metrics.root_mean_squared_error(y_test, y_pred_stack)


perform=get_all_performances(value_train=(y_train, stackreg.predict(X_train)),
                     values_test=(y_test, stackreg.predict(X_test)),
                     metrics=METRICS
                    )
# Sauvegarder dans un fichier
joblib.dump(perform, "../Ressources/performance/stackreg_performance.pkl")





import joblib

joblib.dump(ridge_cv, '../Ressources/modele_final/ridge_model.pkl')
joblib.dump(pipeline, '../Ressources/pipeline/pipeline.pkl')
joblib.dump(cat_cv,'../Ressources/modele_final/cat_model.pkl')
